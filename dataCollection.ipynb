{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) <2022>, <Regina Nockerts>\n",
    "All rights reserved.\n",
    "\n",
    "This source code is licensed under the BSD-style license found in the\n",
    "LICENSE file in the root directory of this source tree. \n",
    "\n",
    "\n",
    "# Twitter scraping with snscrape\n",
    "Thanks to:  <br>\n",
    "> Beck, M. (2022, January 5). How to Scrape Tweets With snscrape. Medium. https://betterprogramming.pub/how-to-scrape-tweets-with-snscrape-90124ed006af\n",
    "\n",
    "First, install snscrape and import needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install snscrape\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random as rand\n",
    "from nlpUtils import aardvark as aa \n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important note on AND and OR in twitter searches:\n",
    "\n",
    "Precedence of AND before OR:<br>\n",
    "<br>\n",
    "One key difference to keep in mind when using operators with the new recent search and filtered stream endpoints is that<br>\n",
    "* In the old v1.1 API (search/tweets), OR is applied before logical AND(which is denoted by a space between terms or operators)\n",
    "* In the new Twitter API (recent search and filtered stream), AND is applied before OR\n",
    "<br>\n",
    "\n",
    "See example below:<br>\n",
    "<br>\n",
    "Query: corona covid OR covid-19<br>\n",
    "Interpretation in old standard search endpoint: <br>\n",
    "* Will return all Tweets with the term corona along with either the term covid or covid-19\n",
    "\n",
    "Interpretation in new recent search endpoint:<br>\n",
    "* Will return all Tweets that either contain:\n",
    "    * both the terms - corona and covid\n",
    "    * or the term covid-19\n",
    "<br>\n",
    "\n",
    "Quoted from: https://developer.twitter.com/en/docs/tutorials/building-high-quality-filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First look at what types of data can be collected\n",
    "\n",
    "Scrape the data to a JSON file via the terminal. <br>\n",
    "This is not the prefered method as it collects personally identifiable information. \n",
    "However, I will use it first to see what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE user queries for the terminal:\n",
    "snscrape --jsonl --progress --max-results 10 --since 2021-06-01 twitter-search \"its the elephant until:2022-07-31\" > text-query-tweets.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we get the following info types: <br>\n",
    "_type; \n",
    "url; \n",
    "date; \n",
    "content (emojis are included in the body of the content as unicode characters - ex: \\ud83d\\udc47); \n",
    "renderedContent (not sure how this is different than the \"content\"); \n",
    "id; \n",
    "user (username, id, displayname, description, rawDescription, descriptionUrls, verified, created, followersCount, friendsCount, statusesCount, favouritesCount, listedCount, mediaCount, location, protected, linkUrl, linkTcourl, profileImageUrl, profileBannerUrl, label, url); \n",
    "replyCount; \n",
    "retweetCount; \n",
    "likeCount; \n",
    "quoteCount; \n",
    "conversationId; \n",
    "lang; \n",
    "source; \n",
    "sourceUrl; \n",
    "sourceLabel; \n",
    "outlinks; \n",
    "tcooutlinks; \n",
    "media; \n",
    "retweetedTweet; \n",
    "quotedTweet; \n",
    "inReplyToTweetId; \n",
    "inReplyToUser; \n",
    "mentionedUsers; \n",
    "coordinates; \n",
    "place; \n",
    "hashtags; \n",
    "cashtags.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_list = []\n",
    "\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper(\"its the elephant until:2022-04-10\").get_items()):\n",
    "    if i > 500:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.content, tweet.renderedContent])\n",
    "\n",
    "tweets_lower = pd.DataFrame(tweets_list, columns=['Datetime', 'Text', 'Rendered Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the search function works\n",
    "Is the search function capitalization neutral? Looks like it, yes. <br>\n",
    "Can I search for the same term with OR and get back the same set of results? Yep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 2)\n",
      "(320, 2)\n",
      "(320, 2)\n",
      "(320, 2)\n",
      "(320, 2)\n",
      "(320, 2)\n"
     ]
    }
   ],
   "source": [
    "tweets_list = []\n",
    "\n",
    "# for i, tweet in enumerate(sntwitter.TwitterSearchScraper(\"(bijltjespad) since:2006-04-01 until:2020-04-02\").get_items()):\n",
    "# for i, tweet in enumerate(sntwitter.TwitterSearchScraper(\"(Bijltjespad) since:2006-04-01 until:2020-04-02\").get_items()):\n",
    "# for i, tweet in enumerate(sntwitter.TwitterSearchScraper(\"(BIJLTJESPAD) since:2006-04-01 until:2020-04-02\").get_items()):\n",
    "# for i, tweet in enumerate(sntwitter.TwitterSearchScraper(\"(BIJLTJESPAD) AND (bijltjespad) since:2006-04-01 until:2020-04-02\").get_items()):\n",
    "# for i, tweet in enumerate(sntwitter.TwitterSearchScraper(\"(BIJLTJESPAD OR bijltjespad) AND (bijltjespad) since:2006-04-01 until:2020-04-02\").get_items()):\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper(\"(BIJLTJESPAD OR bijltjespad) AND (bijltjespad OR Bijltjespad) since:2006-04-01 until:2020-04-02\").get_items()):\n",
    "    if i > 500:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.content])\n",
    "\n",
    "# tweets_lower = pd.DataFrame(tweets_list, columns=['Datetime', 'Content'])\n",
    "print(tweets_lower.shape)\n",
    "\n",
    "# tweets_cap = pd.DataFrame(tweets_list, columns=['Datetime', 'Content'])\n",
    "print(tweets_cap.shape)\n",
    "\n",
    "# tweets_allcap = pd.DataFrame(tweets_list, columns=['Datetime', 'Content'])\n",
    "print(tweets_allcap.shape)\n",
    "\n",
    "# tweets_allcap_lower = pd.DataFrame(tweets_list, columns=['Datetime', 'Content'])\n",
    "print(tweets_allcap_lower.shape)\n",
    "\n",
    "# tweets_allcap_lower_or = pd.DataFrame(tweets_list, columns=['Datetime', 'Content'])\n",
    "print(tweets_allcap_lower_or.shape)\n",
    "\n",
    "tweets_allcap_lower_or_or = pd.DataFrame(tweets_list, columns=['Datetime', 'Content'])\n",
    "print(tweets_allcap_lower_or_or.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can I use incomplete*?\n",
    "\n",
    "Yes, it seems so. But this will return a LOT of junk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ContentClean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>2021-09-10 23:17:49+00:00</td>\n",
       "      <td>@Jessnj4554 @politvidchannel The Afghan price ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>2021-09-10 23:17:39+00:00</td>\n",
       "      <td>When are people going to #WakeUp\\n\\nTake That,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>2021-09-10 23:17:36+00:00</td>\n",
       "      <td>Measles Cases Force US To Suspend Afghan Refug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>2021-09-10 23:17:29+00:00</td>\n",
       "      <td>Joe Biden is guilty of war crimes for bombing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>2021-09-10 23:17:27+00:00</td>\n",
       "      <td>Afghan special forces commando held after arme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Date  \\\n",
       "496 2021-09-10 23:17:49+00:00   \n",
       "497 2021-09-10 23:17:39+00:00   \n",
       "498 2021-09-10 23:17:36+00:00   \n",
       "499 2021-09-10 23:17:29+00:00   \n",
       "500 2021-09-10 23:17:27+00:00   \n",
       "\n",
       "                                          ContentClean  \n",
       "496  @Jessnj4554 @politvidchannel The Afghan price ...  \n",
       "497  When are people going to #WakeUp\\n\\nTake That,...  \n",
       "498  Measles Cases Force US To Suspend Afghan Refug...  \n",
       "499  Joe Biden is guilty of war crimes for bombing ...  \n",
       "500  Afghan special forces commando held after arme...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_list = []\n",
    "\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper(\"(afghan*) since:2021-09-10 until:2021-09-11\").get_items()):\n",
    "    if i > 500:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.content])\n",
    "\n",
    "tweets_temp = pd.DataFrame(tweets_list, columns=['Date', 'ContentClean'])\n",
    "tweets_temp.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa.term_check(\"Afghan\", tweets_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "(142, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ContentClean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>2021-09-10 23:18:40+00:00</td>\n",
       "      <td>Remember a 'double suicide bomb attack' on wai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2021-09-10 23:18:31+00:00</td>\n",
       "      <td>US gives 1st public look inside base housing A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2021-09-10 23:18:28+00:00</td>\n",
       "      <td>So maybe the reason names of the 2 top Afghans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2021-09-10 23:18:20+00:00</td>\n",
       "      <td>.@JoeBiden killed a bunch of innocent Afghans ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2021-09-10 23:17:55+00:00</td>\n",
       "      <td>@DrFeelgood95 Can’t find the words to describe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Date  \\\n",
       "137 2021-09-10 23:18:40+00:00   \n",
       "138 2021-09-10 23:18:31+00:00   \n",
       "139 2021-09-10 23:18:28+00:00   \n",
       "140 2021-09-10 23:18:20+00:00   \n",
       "141 2021-09-10 23:17:55+00:00   \n",
       "\n",
       "                                          ContentClean  \n",
       "137  Remember a 'double suicide bomb attack' on wai...  \n",
       "138  US gives 1st public look inside base housing A...  \n",
       "139  So maybe the reason names of the 2 top Afghans...  \n",
       "140  .@JoeBiden killed a bunch of innocent Afghans ...  \n",
       "141  @DrFeelgood95 Can’t find the words to describe...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "afg_index = []\n",
    "counter = 0\n",
    "\n",
    "tweets_temp.reset_index(drop=True, inplace=True)\n",
    "for i, tweet in enumerate(tweets_temp[\"ContentClean\"]):\n",
    "    if \"afghan \" in tweet.lower():\n",
    "        afg_index.append(i)\n",
    "        counter +=1\n",
    "\n",
    "tweets_temp.drop(afg_index, inplace=True)\n",
    "tweets_temp.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(len(afg_index))\n",
    "print(counter)\n",
    "print(tweets_temp.shape)\n",
    "tweets_temp.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "look = aa.subset_gen(tweets_temp, 20, seed=1080)\n",
    "look.insert(loc=2, column='ContentLabel', value=\"\")\n",
    "\n",
    "# NOTE: This function starts with an input: to reset the index\n",
    "aa.labeler(look, col=\"ContentClean\", lab=\"ContentLabel\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISSUE: Missed Matches\n",
    "**WTF is going on with the searh dropping some rows with a term is added with OR?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 2)\n",
      "(501, 2)\n"
     ]
    }
   ],
   "source": [
    "tweets_list = []\n",
    "\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper(\"(bijltjespad OR kattenburgergracht) AND (bijltjespad OR amsterdam) since:2006-04-01 until:2020-04-02\").get_items()):\n",
    "    if i > 500:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.content])\n",
    "\n",
    "#tweets_no_kat = pd.DataFrame(tweets_list, columns=['Datetime', 'Content'])\n",
    "#print(tweets_no_kat.shape)\n",
    "tweets_kat = pd.DataFrame(tweets_list, columns=['Datetime', 'Content'])\n",
    "print(tweets_kat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: P 1 BAD-04 (Kleine IBGS) Stank/hind. lucht (gaslucht) (meting) , Bijltjespad Amsterdam 13ASN https://t.co/YpWd61Q6h0\n",
      "200: Studentenkamer Amsterdam, Bijltjespad: Amsterdam, Bijltjespad, Deze zomer (rond 30 juni t/m 30 augustus, maar... http://t.co/gBWMCW85lq\n",
      "400: Huisgenoot gratis af te halen aan bijltjespad 30!: Huisgenoot gratis af te halen aan bijltjespad 30! http://tinyurl.com/d8hovh\n",
      "(185, 4)\n",
      "(4, 4)\n",
      "(406, 4)\n"
     ]
    }
   ],
   "source": [
    "# NOTE: This functions starts with input: box\n",
    "# NOTE: This function returns THREE dataframes: superset only AND subset only AND inner/overlap, in that order\n",
    "kat, no_kat, both = aa.outer_df (tweets_kat, tweets_no_kat)\n",
    "print(kat.shape)\n",
    "print(no_kat.shape)\n",
    "print(both.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Calamiteit Amsterdam 04:38 - BR 1 BRANDGERUCHT (Incidentnet: reg.inmeld) BIJLTJESPAD 6 ASD [ 531 ]'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_kat.loc[2, \"Content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: This function opens an input: box\n",
    "aa.labeler(no_kats_df, col=\"Content\", lab=\"ContentLabel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About: missed matches\n",
    "* \"(bijltjespad)\" and \"(bijltjespad) OR (bijltjespad)\" and \"(bijltjespad) AND (bijltjespad)\" all have the same number of rows: 320\n",
    "* \"(bijltjespad) AND (bijltjespad OR amsterdam)\" also has the same number, as expected: 320\n",
    "* \"(bijltjespad OR kattenburgergracht) AND (bijltjespad OR amsterdam)\" has extra rows, as expected: 501\n",
    "    * 320 --> 181 =  extra rows.\n",
    "* BUT when you find the difference between the sets, it is not that simple...\n",
    "    * the set that includes \"kattenburgergracht\" has 185 rows that are not in the smaller set;\n",
    "        * These look like legitimate exclusions from the smaller set.\n",
    "    * the set that does not include \"kattenburgergracht\" has 4 rows that are not in the larger set;\n",
    "        * These look like they **should** have been included in the larger set:\n",
    "            * they all should have matched \"(bijltjespad) AND (bijltjespad)\"\n",
    "            * and should also have matched \"(bijltjespad) AND (amsterdam)\" [NOTE: tried this with a larger set and about half should have matched with amsterdam and half not]\n",
    "    * 185 - 4 = 181 --> the extra rows value.\n",
    "* **So why does adding a term lose us some relevant rows (even if it adds more)?**\n",
    "    * It's not the capitalization\n",
    "    * It's not the since: until:\n",
    "* **More important, what can I do about this?**\n",
    "    * Try running these searches through the terminal with this format:<br>\n",
    "        * snscrape --jsonl --progress --max-results 500000 --since 2021-01-01 twitter-search \"its the elephant until:2022-01-01\" > text-query-tweets.json\n",
    "    * Just note this as a shortcoming of the methodology and move on, at elast for now. This is for \"future work\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content v. Rendered Content?\n",
    "I'm not sure what the difference is between content and rendered content, so I am going to collect 500 tweets and compare those two fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df[\"Same\"] = np.where(tweets_df[\"Text\"] == tweets_df[\"Rendered Text\"], 1, 0)\n",
    "print(sum(tweets_df[\"Same\"])) # that's a lot of different tweets, but not all. \n",
    "\n",
    "print(tweets_df[\"Text\"][7])  # look at one example closer up.\n",
    "print(tweets_df[\"Rendered Text\"][7])\n",
    "\n",
    "tweets_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference appears to be how embedded links are treated. As the first version is still a followable link, I will preserve that version of the tweet text.\n",
    "<br><br>\n",
    "We will do the actual scraping from the Python wrapper, specifying which fields to save and thus excluding much personally identifiable information.\n",
    "<br>\n",
    "So we will want to collect: <br>\n",
    "* date;\n",
    "* content (including emojis as unicode characters - ex: \\ud83d\\udc47); \n",
    "* user.followersCount; \n",
    "* user.friendsCount; \n",
    "* user.location; \n",
    "* replyCount; \n",
    "* retweetCount; \n",
    "* likeCount; \n",
    "* quoteCount; \n",
    "* lang; \n",
    "* retweetedTweet; \n",
    "* quotedTweet; \n",
    "* coordinates; \n",
    "* place; \n",
    "* hashtags; \n",
    "* cashtags.\n",
    "\n",
    "## Baseline search\n",
    "But for now, let's take some small tweet sets and refine search terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_list = []\n",
    "\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper('(Afghanistan OR Afghan OR Afghani OR withdrawal OR war OR resettle OR resettlement) AND (\"come here\" OR migrant OR immigrant OR refugee OR asylum OR resettle OR resettlement) lang:en since:2021-08-16 until:2021-08-17').get_items()):\n",
    "    if i > 500000:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.content])\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=[\"Date\", \"Content\"])\n",
    "tweets_df.to_csv(os.path.join('archiveData','hannos_test_tweets.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_csv(os.path.join('archiveData','2021-09-01_2022-02-02_FIRSTpass_tweets.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Date                                            Content\n",
      "0 2021-09-01 23:59:04+00:00  @BaphoNetArchive I'm from Texas originally, an...\n",
      "1 2021-09-01 23:57:21+00:00  @SharonP92453996 @kelliwardaz @LyndsayMKeith @...\n",
      "2 2021-09-01 23:56:18+00:00  NEW: Most Afghan evacuees will arrive to the U...\n",
      "3 2021-09-01 23:52:10+00:00  @Cristiano I am the son of an Afghanistan immi...\n",
      "4 2021-09-01 23:51:10+00:00  So the left was ok with allowing over 100k ref...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2206</th>\n",
       "      <td>2021-09-01 00:01:51+00:00</td>\n",
       "      <td>Efforts to resettle people here continue. Ange...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2207</th>\n",
       "      <td>2021-09-01 00:01:43+00:00</td>\n",
       "      <td>One question that I’ll be mulling over in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2208</th>\n",
       "      <td>2021-09-01 00:00:58+00:00</td>\n",
       "      <td>Native people from Afghanistan lawfully reside...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2209</th>\n",
       "      <td>2021-09-01 00:00:25+00:00</td>\n",
       "      <td>People who wanted to sanction Pakistan, now wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2210</th>\n",
       "      <td>2021-09-01 00:00:22+00:00</td>\n",
       "      <td>Franklin County commissioners adopted a resolu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Date  \\\n",
       "2206 2021-09-01 00:01:51+00:00   \n",
       "2207 2021-09-01 00:01:43+00:00   \n",
       "2208 2021-09-01 00:00:58+00:00   \n",
       "2209 2021-09-01 00:00:25+00:00   \n",
       "2210 2021-09-01 00:00:22+00:00   \n",
       "\n",
       "                                                Content  \n",
       "2206  Efforts to resettle people here continue. Ange...  \n",
       "2207  One question that I’ll be mulling over in the ...  \n",
       "2208  Native people from Afghanistan lawfully reside...  \n",
       "2209  People who wanted to sanction Pakistan, now wa...  \n",
       "2210  Franklin County commissioners adopted a resolu...  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tweets_df.head())\n",
    "tweets_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, yeah, one day in the middle of August, before Kabul fell, there were about 2,000+ tweets. Is this a lot? It seems fine, but somewhat less than I expected. Enough to do interesting things with. But I am concerned that I am missing most of the twitter conversation that I am interested in, which limits my policy relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Search Terms\n",
    "So, this is actually quite hard. \n",
    "\n",
    "I started with a set of terms from my own knowledge, background literature on refugees and twitter sentiment, and major American news sources (Washington post, Vox, Axios). I added terms found via right-leaning news sources (National Review, Fox News, Washingtonian Free Beacon, and The American Thinker). And then reviewed some left-leaning sites (Slate, NPR, Mother Jones, CommonDreams) to ensure that a fair cross section of language was referenced when selecting terms.\n",
    "\n",
    "__________\n",
    "I can also try reviewing the poll questions.\n",
    "____________\n",
    "\n",
    "I want to add terms to my base set such that: \n",
    "* the terms add **relevant** tweets to the dataset\n",
    "* the terms do not add irrelevant tweets to the dataset\n",
    "    * which requires comparing the difference between the new (superset) and old (subset) searches and looking at randomly selected tweets\n",
    "* the dataset ends up broadly representative of the true stance/sentiment expressed on Twitter\n",
    "    * Which is hard, because I do not know what the true values are.\n",
    "\n",
    "I can stop adding terms when adding a new term: \n",
    "* does not add more rows to the dataset ~ irrelevant, my existing terms do the same work, or\n",
    "* adds a large percentage of irrelevant rows to the dataset ~ counterproductive.\n",
    "\n",
    "\n",
    "Now we can look at some potential sets of terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Log\n",
    "\"refugee OR migrant OR immigrant OR asylum OR afghanistan OR afghan OR afghani since:2021-04-20 until:2021-05-01\" <br>\n",
    "Plus the same terms for 04-14 to 04-20 <br>\n",
    "Looking at the tweets from April 2021, they seem to be all over the place. There are no clear trends. And relatively few of them are actually about the idea of increased movement of Afghani nationals. Perhaps it is too early in the timeline - people have not yet recognized the scale of the need nor the impossibly short timeline?\n",
    "______\n",
    "After moving to mid-August (since:2021-08-15 until:2021-08-20), there are far more tweets - I hit max iter (500,000, 273 minutes) after ~1.5 days.\n",
    "<br><br>\n",
    "Now I'm curious about a different search term for April: \"withdrawal.\" This definitely got more results (19,157), but more totally irrelevnat ones and more related to troops but not refugees. \n",
    "<br><br>\n",
    "Maybe \"evacuation OR allies OR resettle OR resettlement\"? Well, \"allies\" gets too many irrelevant hits (gender and race related). And \"evacuation\" is almost all natural disaster-related.\n",
    "____\n",
    "\"(Afghanistan OR Afghan OR Afghani OR withdrawal OR war) AND (migrant OR immigrant OR refugee OR resettle OR resettlement) lang:en since:2021-04-14 until:2021-04-17\" <br>\n",
    "This is getting pretty close. I'm going to try it for one day in mid-August. Nope, too specific, I think. Needs to broaden.<br>\n",
    "<br>\n",
    "11 April 2022<br>\n",
    "'(Afghanistan OR Afghan OR Afghani OR withdrawal OR war OR resettle OR resettlement) AND (\"come here\" OR migrant OR immigrant OR refugee OR asylum OR resettle OR resettlement) lang:en since:2021-08-19 until:2021-08-20'<br>\n",
    "<br>\n",
    "This is starting to look pretty good. Average tweets per day are lower than expected, though. For context, in mid August 2021, the duling hashtags #IStandWithBiden and #BidenDisaster were trending with ~50,000 tweets per day. On the same day (Aug. 16 to 17), \n",
    "In total for the year, my search returns 267,344 (275,588 without lang:en).\n",
    "___\n",
    "Next search: \n",
    "(Afghanistan OR Afghan OR Afghans OR Afghani OR withdrawal OR war OR \"Afghan evacuation\" OR resettle OR resettlement OR resettled) AND \n",
    "(\"come here\" OR migrant OR immigrant OR refugee OR asylum OR vetted OR vetting OR unvetted OR \"without identification\" OR \"lack identification\" OR resettle OR resettlement OR resettled) \n",
    "lang:en since:2021-01-01 until:2022-01-01'\n",
    "<br><br>\n",
    "Search returns: tweet_data_14_04, 319,915 rows (increase of 52,571)<br><br>\n",
    "This was an improvement. I reviewed a random subset of the new rows and they look mostly relevant and new; they tend towards the anti-refugee side, as expected, though there are quite a few pro.\n",
    "\n",
    "___\n",
    "(Afghanistan OR Afghan OR Afghans OR Afghani OR withdrawal OR evacuation OR resettle OR resettlement OR resettled OR \"humanitarian parole\") AND (migrant OR immigrant OR refugee OR asylum OR vetted OR vetting OR unvetted OR \"without identification\" OR \"lack identification\" OR \"lacking identification\" OR resettle OR resettlement OR resettled OR \"humanitarian parole\") lang:en since:2021-01-01 until:2022-01-01\n",
    "<br><br>\n",
    "search 'tweet_data_16_04.csv' returns 270,015 rows<br>\n",
    "Consider putting \"come here\" back into the second search term.\n",
    "\n",
    "___\n",
    "(Afghanistan OR Afghan OR Afghans OR Afghani OR withdrawal OR evacuation OR resettle OR resettlement OR resettled OR \"humanitarian parole\") AND (\"come here\" OR migrant OR immigrant OR refugee OR asylum OR vetted OR vetting OR unvetted OR \"without identification\" OR \"lack identification\" OR \"lacking identification\" OR resettle OR resettlement OR resettled OR \"humanitarian parole\") lang:en since:2021-01-01 until:2022-01-01\n",
    "\n",
    "Search 'tweet_data_17_04.csv' returns 273,721 rows\n",
    "<br><br>\n",
    "Compared to search tweet_data_16_04: \n",
    "* in 16_04 and not 17_04: 1122 rows: Looked at 100 tweets from the difference: 10 were irrelevant, 40 were con, 28 were pro.\n",
    "    * I'm not sure why this isn't empty. The second search only added terms, it did not remove any, so it should only have added matches...\n",
    "* in 17_04 and not 16_04: 4832 rows: mostly relevant; mostly con, some pro.\n",
    "\n",
    "\n",
    "Search 'tweet_data_17b_04.csv' returns 274,425 rows\n",
    "* adds \"to vet\"\n",
    "* 205 more rows than Search 'tweet_data_17_04.csv' <br>\n",
    "--> Keep it in.\n",
    "\n",
    "___\n",
    "Search \"tweet_data_18_04.csv\" returns 274,281 rows:\n",
    "* add: \"Afghan allies\" (1st term)\n",
    "* Compared to 17b_04:\n",
    "    * adds 154 mainly relevant rows (18/25 checked)\n",
    "    * But also kicks out 289 relevant rows (19/25 checked)\n",
    "    * the 'superset' has 144 FEWER rows <br>\n",
    "--> Do not keep\n",
    "\n",
    "\n",
    "Search \"tweet_data_18b_04.csv\" returns 300,446 rows:\n",
    "* remove: \"Afghan allies\"\n",
    "* add: \"women and girls\" (2nd term)\n",
    "* Compared to \"tweet_data_17b_04.csv\":\n",
    "    * Adds 26,554 rows, mainly irrelevant (38 / 50 checked)\n",
    "        * this is mainly general expressions of pity, but little about resettlement specifically\n",
    "    * Drops 525 rows, mainly relevant (20 / 25 checked) <br>\n",
    "--> Do not keep\n",
    "\n",
    "_________\n",
    "\n",
    "Search \"tweet_data_19_04.csv\" returns 290,659 rows:\n",
    "* remove: \"women and girls\"\n",
    "* add: \"SIV\" OR \"Special Immigrant Visa\" (2nd term)\n",
    "* Compared to: \"tweet_data_17b_04.csv\":\n",
    "    * Adds 16,761 rows, mainly relevant (49 / 50 checked)\n",
    "    * Drops 527 rows, mainly relevant (20 / 25 checked)<br>\n",
    "--> Keep\n",
    "\n",
    "Search \"tweet_data_19b_04.csv\" returns 293,865 rows:\n",
    "* add: relocation (2nd term)\n",
    "* Compared to \"tweet_data_19_04.csv\":\n",
    "    * adds 3,507 rows, mostly relevant (41 / 50 checked)\n",
    "    * drops 301 rows, mostly relevant (19 / 25 checked) <br>\n",
    "--> keep\n",
    "\n",
    "_________\n",
    "Search \"tweet_data_25_04.csv\" returns       rows:\n",
    "* add: \"afgan\", \"afgani\"\n",
    "* remove: \"come here\"\n",
    "* Compare to \"tweet_data_19b_04.csv\":\n",
    "    * total: 3842 fewer rows\n",
    "    * adds 1990 rows, mixed relevance\n",
    "    * drops 5818 rows, mostly relevant\n",
    "--> do not keep\n",
    "\n",
    "\n",
    "Search \"tweet_data_25b_04.csv\" returns       rows:\n",
    "* add: \"come here\"\n",
    "* remove: \"afgani\"\n",
    "* Compare to \"tweet_data_19b_04.csv\":\n",
    "    * total: + 511 rows\n",
    "    * adds 1935 rows, 39 / 50 relevant (78%)\n",
    "    * drops 1409 rows, 18 / 20 relevant (90%) <br>\n",
    "--> Do not keep. Back to tweet_data_19b_04.\n",
    "\n",
    "\n",
    "_________\n",
    "Search \"tweet_data_26_04.csv\" returns 371,751 rows:\n",
    "* EXTEND scrape until: 2022-04-02\n",
    "    * to see if trend returns to baseline\n",
    "* Compare to \"tweet_data_19b_04.csv\":\n",
    "    * adds 79,265 rows, 28 / 50 relevant\n",
    "    * drops 2007 rows, 19 / 25 relevant\n",
    "-> this adds a lot of irrelevant rows, as expected. Still useful for determiing the baseline.\n",
    "\n",
    "___________\n",
    "\n",
    "Search \"tweet_data_27_04.csv\" returns 303,846 rows (9981 more rows):\n",
    "* NOT extended scrape, until: 2022-01-01\n",
    "* add: translators\n",
    "* remove: \"come here\"\n",
    "* Compare to \"tweet_data_19b_04.csv\":\n",
    "    * add: 16,869 rows, 49 / 50 relevant\n",
    "    * remove: 6,872 rows, 17 / 24 relevant\n",
    "--> keep\n",
    "\n",
    "\n",
    "\n",
    "Search \"tweet_data_27b_04.csv\" returns 384,555 rows (90,690 more rows):\n",
    "* EXTEND scrape, until: 2022-05-01\n",
    "* Compare to \"tweet_data_27_04.csv\":\n",
    "\n",
    "\n",
    "* Compare to \"tweet_data_19b_04.csv\":\n",
    "    * add: 96,968 rows, 35 / 50 relevant (70%)\n",
    "    * remove: 6,912 rows, 18 / 25 relevant (~72%)\n",
    "\n",
    "--> USE THIS ONE\n",
    "______\n",
    "\n",
    "Yet to be considered: \n",
    "* \"Afghan translator/s\"\n",
    "* influx\n",
    "* \"refugee crisis\"\n",
    "* \"screening process\"\n",
    "* \"foreign nationals\"\n",
    "* airlift / airlifted\n",
    "\n",
    "POSSIBLE: afgha*\n",
    "* this will return a lot of junk, as well as more hits. Where is the tradeoff?\n",
    "\n",
    "\n",
    "Rejected:\n",
    "* \"afghan allies\"\n",
    "* \"women and girls\"\n",
    "* \"afgan\", \"afgani\"\n",
    "* \"come here\" NOTE: this is a good term, but I'm out of space in the search...\n",
    "\n",
    "Other terms that were considered: \n",
    "* visa\n",
    "* crisis\n",
    "* humanitarian\n",
    "* war"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCLUDES UNUSED COLUMNS\n",
    "# tweets_list = []\n",
    "\n",
    "# for i, tweet in enumerate(sntwitter.TwitterSearchScraper('(Afghanistan OR Afghan OR Afghans OR Afghani OR withdrawal OR evacuation OR resettle OR resettlement OR resettled OR \"humanitarian parole\") AND (migrant OR immigrant OR refugee OR asylum OR vetted OR vetting OR unvetted OR \"without identification\" OR \"lack identification\" OR \"lacking identification\" OR resettle OR resettlement OR resettled OR \"humanitarian parole\") lang:en since:2021-01-01 until:2022-01-01').get_items()):\n",
    "#     if i > 500000:\n",
    "#         break\n",
    "#     if tweet.content.startswith(\"rt @\"):\n",
    "#         continue\n",
    "#     else:\n",
    "#         tweets_list.append([tweet.date, tweet.content, tweet.user.followersCount, tweet.user.friendsCount, tweet.user.location, \\\n",
    "#             tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.lang, tweet.retweetedTweet, \\\n",
    "#                 tweet.quotedTweet, tweet.coordinates, tweet.place, tweet.hashtags])\n",
    "\n",
    "# tweets_df = pd.DataFrame(tweets_list, columns=[\"Date\", \"Content\", \"FollowersCount\", \"FriendsCount\", \"Location\", \"ReplyCount\", \"RetweetCount\", \"LikeCount\", \\\n",
    "#     \"QuoteCount\", \"Lang\", \"RetweetedTweet\", \"QuotedTweet\", \"Coordinates\", \"Place\", \"Hashtags\"])\n",
    "\n",
    "# tweets_df.to_csv(os.path.join('archiveData','temp_full.csv'))\n",
    "# tweets_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: I think there is a max number of terms and I have reached it...\n",
    "* replaced \"come here\" with \"Afgani\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_list = []\n",
    "\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper('(Afghanistan OR Afghan OR Afghans OR Afghani OR withdrawal OR evacuation OR resettle OR resettlement OR resettled OR \"humanitarian parole\") AND (translators OR migrant OR immigrant OR refugee OR asylum OR \"SIV\" OR \"Special Immigrant Visa\" OR vetted OR vetting OR unvetted OR \"to vet\" OR \"without identification\" OR \"lack identification\" OR \"lacking identification\" OR relocation OR resettle OR resettlement OR resettled OR \"humanitarian parole\") lang:en since:2021-01-01 until:2022-05-01').get_items()):\n",
    "    if i > 500000:\n",
    "        break\n",
    "    if i % 10000 == 0:\n",
    "        print(\"row:\", i)\n",
    "\n",
    "    if tweet.content.startswith(\"rt @\"):\n",
    "        continue\n",
    "    else:\n",
    "        tweets_list.append([tweet.date, tweet.content, tweet.user.location, \\\n",
    "            tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.hashtags])\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=[\"Date\", \"Content\", \"Location\", \"ReplyCount\", \"RetweetCount\", \"LikeCount\", \\\n",
    "    \"QuoteCount\", \"Hashtags\"])\n",
    "# temp removed columns: , \"FollowersCount\", \"FriendsCount\"  //  tweet.user.followersCount, tweet.user.friendsCount, \n",
    "# Removed columns: \"Lang\", \"RetweetedTweet\", \"QuotedTweet\", \"Coordinates\", \"Place\"\n",
    "\n",
    "tweets_df.to_csv(os.path.join('archiveData','temp_full.csv'))\n",
    "tweets_df.tail()\n",
    "print(tweets_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- EXPORT single search -----------\n",
    "#tweets_df = tweets_df.drop_duplicates().reset_index(drop=True)\n",
    "tweets_df.to_csv(os.path.join('archiveData','tweet_data_27b_04.csv'))\n",
    "\n",
    "# --------- phased search -----------\n",
    "# tweets_df.to_csv(os.path.join('archiveData','2021-09-01_2022-01-01_FIRSTpass_tweets.csv'))\n",
    "# tweets_df.to_csv(os.path.join('archiveData','2021-05-01_2021-09-01_FIRSTpass_tweets.csv'))\n",
    "# tweets_df.to_csv(os.path.join('archiveData','2021-01-01_2021-05-01_FIRSTpass_tweets.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- IMPORT single search -----------\n",
    "tweets_df = pd.read_csv(os.path.join('archiveData',\"tweet_data_27b_04.csv\"), header=0, index_col=0)\n",
    "print(tweets_df.info(show_counts=True))\n",
    "\n",
    "# --------- phased search -----------\n",
    "# create a large  dataframe with the full year's worth of tweets\n",
    "# tweets_data1 = pd.read_csv(os.path.join('archiveData',\"2021-01-01_2021-05-01_FIRSTpass_tweets.csv\"), header=0, index_col=0)\n",
    "# tweets_data2 = pd.read_csv(os.path.join('archiveData',\"2021-05-01_2021-09-01_FIRSTpass_tweets.csv\"), header=0, index_col=0)\n",
    "# tweets_data3 = pd.read_csv(os.path.join('archiveData',\"2021-09-01_2022-02-02_FIRSTpass_tweets.csv\"), header=0, index_col=0)\n",
    "\n",
    "# frames = [tweets_data3, tweets_data2, tweets_data1]\n",
    "# tweets_data = pd.concat(frames, ignore_index=True)\n",
    "# tweets_data = tweets_data.drop_duplicates().reset_index(drop=True)\n",
    "# tweets_data.to_csv(os.path.join('archiveData',\"tweet_data.csv\"))\n",
    "# print(tweets_df.info(show_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the difference a term makes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub = pd.read_csv(os.path.join('archiveData',\"tweet_data_clean.csv\"), header=0, index_col=0)\n",
    "\n",
    "# NOTE: This function starts with an input: box\n",
    "diff_14_04 = aa.outer_df (tweets_df, sub, silent=\"no\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(319916, 15)\n",
      "a dataframe and .csv of length 100 have been created\n"
     ]
    }
   ],
   "source": [
    "print(tweets_df.shape)\n",
    "my_a = aa.subset_gen(tweets_df, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superset: (303846, 8)\n",
      "subset (293865, 10)\n",
      "the superset has 9981 more rows.\n",
      "\n",
      "just in superset: (16869, 18)\n",
      "just in subset: (6872, 18)\n"
     ]
    }
   ],
   "source": [
    "big = pd.read_csv(os.path.join('data',\"tweet_data_27_04.csv\"), header=0, index_col=0)\n",
    "small =  pd.read_csv(os.path.join('archiveData',\"tweet_data_19b_04.csv\"), header=0, index_col=0)\n",
    "\n",
    "# NOTE: This functions starts with input: box\n",
    "# NOTE: This function returns THREE dataframes: superset only AND subset only AND inner/overlap, in that order\n",
    "big_d, small_d, both = aa.outer_df (big, small, silent=\"yes\")\n",
    "\n",
    "print(\"superset:\", big.shape)\n",
    "print(\"subset\", small.shape)\n",
    "print(\"the superset has\", big.shape[0] - small.shape[0], \"more rows.\")\n",
    "print()\n",
    "print(\"just in superset:\", big_d.shape)\n",
    "print(\"just in subset:\", small_d.shape)\n",
    "#print(\"in both super- and subset:\", both.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a dataframe and temp_subset_gen.csv of length 50 have been created\n",
      "a dataframe and temp_subset_gen.csv of length 25 have been created\n"
     ]
    }
   ],
   "source": [
    "my_big_d = aa.subset_gen(big_d, 50)\n",
    "my_small_d = aa.subset_gen(small_d, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: This function opens an input: box\n",
    "aa.labeler(my_big_d, col=\"Content\", lab=\"ContentLabel\")\n",
    "aa.labeler(my_small_d, col=\"Content\", lab=\"ContentLabel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y    49\n",
      "n     1\n",
      "Name: ContentLabel, dtype: int64\n",
      "y    17\n",
      "n     7\n",
      "Name: ContentLabel, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(my_big_d[\"ContentLabel\"].value_counts())\n",
    "print(my_small_d[\"ContentLabel\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save final data set\n",
    "'tweet_data_27b_04.csv' is the final data set. Save it to data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_csv(os.path.join('data','tweet_data_27b_04.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sntwitter.TwitterSearchScraper( <br>\n",
    "    '(Afghanistan OR Afghan OR Afghans OR Afghani OR withdrawal OR evacuation OR resettle OR resettlement OR resettled OR \"humanitarian parole\") <br>\n",
    "    AND <br>\n",
    "    (translators OR migrant OR immigrant OR refugee OR asylum OR \"SIV\" OR \"Special Immigrant Visa\" OR vetted OR vetting OR unvetted OR \"to vet\" OR \"without identification\" OR \"lack identification\" OR \"lacking identification\" OR relocation OR resettle OR resettlement OR resettled OR \"humanitarian parole\") <br>\n",
    "    lang:en since:2021-01-01 until:2022-05-01').get_items())<br>\n",
    "\n",
    "\n",
    "\n",
    "So far:\n",
    "* Search term: (Afghanistan OR Afghan OR Afghani OR withdrawal OR war OR resettle OR resettlement) AND (\"come here\" OR migrant OR immigrant OR refugee OR asylum OR resettle OR resettlement)\n",
    "* Do include lang:en in the search\n",
    "* Remove: -Gotham (4 rows) -Arkham (292 rows)\n",
    "* check for and skip \"rt @...\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d5a672f32d72b7000e11999081d09a97571b10f2df9aaee5f232791dc820369"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
