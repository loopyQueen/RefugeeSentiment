{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) <2022>, <Regina Nockerts>\n",
    "All rights reserved.\n",
    "\n",
    "This source code is licensed under the BSD-style license found in the\n",
    "LICENSE file in the root directory of this source tree. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Baseline\n",
    "I am using the majority class prediction as the baseline for this model. From labeling, cleaning, and balancing, we know that negative is the majority class: Majority class = negative = 0\n",
    "\n",
    "IN ADDITION: remember that our evaluation metric will be __F1 and AUC__. Create a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the test dataset\n",
    "x_test = pd.read_csv(\"dataBalancedSets/x_test.csv\", header=0, index_col=0)\n",
    "y_test = pd.read_csv(\"dataBalancedSets/y_test_sent.csv\", header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the groundtruth - For all of them...\n",
    "true = list(y_train_sent[\"y_sent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Function which takes two lists and a reference indicating which class to calculate the TP, FP, and FN for.\n",
    "# aa.classConfScores(y_true, y_pred, reference)\n",
    "\n",
    "# NOTE: A function which takes the precision and recall of some model, and a value for beta, and returns the f_beta-score\"\"\"\n",
    "# aa.fBetaScore(precision, recall, beta=1)\n",
    "\n",
    "# Make a list of 1s as long as the training set to function as our base prediction.\n",
    "maj_pred = [1 for i in range(len(y_train_sent))]\n",
    "\n",
    "## Find the microaverage of the F1 scores for the balseline prediction\n",
    "majority_microF1 = f1_score(y_true=true, y_pred=maj_pred, average='micro', zero_division='warn')\n",
    "majority_macroF1 = f1_score(y_true=true, y_pred=maj_pred, average='macro', zero_division='warn')\n",
    "\n",
    "### Get per-class values (only works for the prediced class right now - division by zero)\n",
    "# Use that list to find the confusion matrix values.\n",
    "(TP, FP, FN) = aa.classConfScores(true, maj_pred, 1)\n",
    "# Use the confusion scores to find precision, recall, and the F1 score\n",
    "p = TP / (TP + FP)\n",
    "r = TP / (TP + FN)\n",
    "f = aa.fBetaScore(precision=p, recall=r, beta=1)  # takes the precision and recall of some model, and a value for beta\n",
    "\n",
    "### Print it all out\n",
    "print(\"Micro- and Macro-Average\")\n",
    "print('\\tMajority class prediction F-score, micro average: {:04.3f}'.format(majority_microF1))\n",
    "print('\\tMajority class prediction F-score, macro average: {:04.3f}'.format(majority_macroF1))\n",
    "print('')\n",
    "print('class: {:d}'.format(1))\n",
    "print('\\tPrecision: {:04.3f}'.format(p))\n",
    "print('\\tRecall: {:04.3f}'.format(r))\n",
    "print('\\tF-score: {:04.3f}'.format(f))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2522cab69aef9135531abc74cfe3f2456cb406a72442e0865122b8d4f66eb9dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
