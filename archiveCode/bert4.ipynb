{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nlpUtils import aardvark as aa \n",
    "from numpy import random as rand\n",
    "from sklearn.metrics import f1_score, roc_curve, auc\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()  # Turns off the warning when you load the model without training\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [\"this sucks\", \"the long dark nightime of the soul\", \"oh happy day\", \"this is just sunshine and rainbows\", \"piss on you and the horse you rode in on\", \"this is so\"]\n",
    "X_val = [\"my kingdom for a focking horse\", \"this is not what i had hoped for\", \"my heart sings with joy\", \"i am not happy\", \"i am happy\", \"the sky is blue\"]\n",
    "Y_train = [0, 0, 1, 1, 0, 2]\n",
    "Y_val = [0, 0, 1, 0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "#tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=40)\n",
    "X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THANKS TO: https://towardsdatascience.com/fine-tuning-pretrained-nlp-models-with-huggingfaces-trainer-6326a4456e7b\n",
    "# Create torch dataset \n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "# Define Trainer parameters\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "train_dataset = Dataset(X_train_tokenized, Y_train)\n",
    "val_dataset = Dataset(X_val_tokenized, Y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rnocker\\Anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 6\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rnocker\\Desktop\\python\\THESIS\\bert4.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rnocker/Desktop/python/THESIS/bert4.ipynb#ch0000006?line=12'>13</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rnocker/Desktop/python/THESIS/bert4.ipynb#ch0000006?line=13'>14</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rnocker/Desktop/python/THESIS/bert4.ipynb#ch0000006?line=14'>15</a>\u001b[0m     args\u001b[39m=\u001b[39margs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rnocker/Desktop/python/THESIS/bert4.ipynb#ch0000006?line=18'>19</a>\u001b[0m     callbacks\u001b[39m=\u001b[39m[EarlyStoppingCallback(early_stopping_patience\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rnocker/Desktop/python/THESIS/bert4.ipynb#ch0000006?line=19'>20</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rnocker/Desktop/python/THESIS/bert4.ipynb#ch0000006?line=20'>21</a>\u001b[0m \u001b[39m# model.resize_token_embeddings(len(tokenizer))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rnocker/Desktop/python/THESIS/bert4.ipynb#ch0000006?line=21'>22</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rnocker/Desktop/python/THESIS/bert4.ipynb#ch0000006?line=22'>23</a>\u001b[0m \u001b[39m# Train pre-trained model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rnocker/Desktop/python/THESIS/bert4.ipynb#ch0000006?line=23'>24</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\rnocker\\Anaconda3\\lib\\site-packages\\transformers\\trainer.py:1317\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/rnocker/Anaconda3/lib/site-packages/transformers/trainer.py?line=1311'>1312</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   <a href='file:///c%3A/Users/rnocker/Anaconda3/lib/site-packages/transformers/trainer.py?line=1313'>1314</a>\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   <a href='file:///c%3A/Users/rnocker/Anaconda3/lib/site-packages/transformers/trainer.py?line=1314'>1315</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   <a href='file:///c%3A/Users/rnocker/Anaconda3/lib/site-packages/transformers/trainer.py?line=1315'>1316</a>\u001b[0m )\n\u001b[1;32m-> <a href='file:///c%3A/Users/rnocker/Anaconda3/lib/site-packages/transformers/trainer.py?line=1316'>1317</a>\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   <a href='file:///c%3A/Users/rnocker/Anaconda3/lib/site-packages/transformers/trainer.py?line=1317'>1318</a>\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   <a href='file:///c%3A/Users/rnocker/Anaconda3/lib/site-packages/transformers/trainer.py?line=1318'>1319</a>\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   <a href='file:///c%3A/Users/rnocker/Anaconda3/lib/site-packages/transformers/trainer.py?line=1319'>1320</a>\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   <a href='file:///c%3A/Users/rnocker/Anaconda3/lib/site-packages/transformers/trainer.py?line=1320'>1321</a>\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   <a href='file:///c%3A/Users/rnocker/Anaconda3/lib/site-packages/transformers/trainer.py?line=1321'>1322</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\rnocker\\Anaconda3\\lib\\site-packages\\transformers\\trainer.py:1556\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/rnocker/Anaconda3/lib/site-packages/transformers/trainer.py?line=1552'>1553</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/rnocker/Anaconda3/lib/site-packages/transformers/trainer.py?line=1553'>1554</a>\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m-> <a href='file:///c%3A/Users/rnocker/Anaconda3/lib/site-packages/transformers/trainer.py?line=1555'>1556</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   <a href='file:///c%3A/Users/rnocker/Anaconda3/lib/site-packages/transformers/trainer.py?line=1556'>1557</a>\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   <a href='file:///c%3A/Users/rnocker/Anaconda3/lib/site-packages/transformers/trainer.py?line=1557'>1558</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   <a href='file:///c%3A/Users/rnocker/Anaconda3/lib/site-packages/transformers/trainer.py?line=1558'>1559</a>\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   <a href='file:///c%3A/Users/rnocker/Anaconda3/lib/site-packages/transformers/trainer.py?line=1559'>1560</a>\u001b[0m ):\n\u001b[0;32m   <a href='file:///c%3A/Users/rnocker/Anaconda3/lib/site-packages/transformers/trainer.py?line=1560'>1561</a>\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/rnocker/Anaconda3/lib/site-packages/transformers/trainer.py?line=1561'>1562</a>\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   <a href='file:///c%3A/Users/rnocker/Anaconda3/lib/site-packages/transformers/trainer.py?line=1562'>1563</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "## THANKS TO: https://towardsdatascience.com/fine-tuning-pretrained-nlp-models-with-huggingfaces-trainer-6326a4456e7b\n",
    "# Define Trainer\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    seed=0,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Train pre-trained model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2522cab69aef9135531abc74cfe3f2456cb406a72442e0865122b8d4f66eb9dc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
