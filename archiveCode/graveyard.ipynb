{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to folder in same directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os.path\n",
    "df = ({'Yes': [12, 45], 'No': [2, 55]})\n",
    "df.to_csv(os.path.join('archiveData','thisTest.csv'))\n",
    "# C:\\Users\\rnocker\\Desktop\\python\\THESIS\\archiveData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Yes</th>\n",
       "      <th>No</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Yes  No\n",
       "0   12   2\n",
       "1   45  55"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(os.path.join('archiveData','thisTest.csv'), header=0, index_col=0)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two plots right next to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[\"y_bert\"].value_counts())\n",
    "print(y_val[\"y_bert\"].value_counts())\n",
    "\n",
    "plt.figure(figsize=(4, 12))\n",
    "\n",
    "fig, ax =plt.subplots(1,2)\n",
    "sns.countplot(x=y_train[\"y_bert\"], palette=my_plot_colors, ax=ax[0]).set_xticklabels([\"neg\", \"neutral\", \"pos\"])\n",
    "ax[0].set_title('Class Distribution, \\n training data')\n",
    "\n",
    "sns.countplot(x=y_val[\"y_bert\"], palette=my_plot_colors, ax=ax[1], ).set_xticklabels([\"neg\", \"neutral\", \"pos\"])\n",
    "ax[1].set_title('Class Distribution, \\n validation data')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is', 'my')\n",
      "('is', 'my', 'sentence;')\n",
      "('my', 'sentence;', 'I')\n",
      "('sentence;', 'I', 'love')\n",
      "('I', 'love', 'it')\n",
      "('love', 'it', 'so.')\n"
     ]
    }
   ],
   "source": [
    "from nltk import bigrams\n",
    "from nltk import ngrams\n",
    "\n",
    "text = \"this is my sentence; I love it so.\"\n",
    "a = list(ngrams(text.split(), 3))\n",
    "for grams in a:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do I need lang:en in my search?\n",
    "\n",
    "What happens if we do not specify the lang:en?\n",
    "Turns out, not a good approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_list = []\n",
    "\n",
    "# for i, tweet in enumerate(sntwitter.TwitterSearchScraper('(Afghanistan OR Afghan OR Afghani OR withdrawal OR war OR resettle OR resettlement) AND (\"come here\" OR migrant OR immigrant OR refugee OR asylum OR resettle OR resettlement) since:2021-01-01 until:2022-01-01').get_items()):\n",
    "#     if i > 500000:\n",
    "#         break\n",
    "#     tweets_list.append([tweet.date, tweet.content, tweet.user.followersCount, tweet.user.friendsCount, tweet.user.location, \\\n",
    "#         tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.lang, tweet.retweetedTweet, \\\n",
    "#             tweet.quotedTweet, tweet.coordinates, tweet.place, tweet.hashtags])\n",
    "\n",
    "# tweets_df = pd.DataFrame(tweets_list, columns=[\"Date\", \"Content\", \"FollowersCount\", \"FriendsCount\", \"Location\", \"ReplyCount\", \"RetweetCount\", \"LikeCount\", \\\n",
    "#     \"QuoteCount\", \"Lang\", \"RetweetedTweet\", \"QuotedTweet\", \"Coordinates\", \"Place\", \"Hashtags\"])\n",
    "\n",
    "# tweets_df.to_csv('temp_full.csv')\n",
    "# tweets_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below: creates a df with all unique rows and looks at a few of those rows. It seems like there are a few of thses tweets that are in English, but the vast majority are not. So this is not useable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_diff = tweets_data.merge(tweets_df, on='Content', how='outer', indicator=True)\n",
    "tweets_diff = tweets_diff.loc[tweets_diff[\"_merge\"] == \"right_only\"]\n",
    "for i, tweet in enumerate(tweets_diff[\"Content\"]):\n",
    "    if i % 200 == 0:\n",
    "        print(\"{}: {}\".format(i, tweet))\n",
    "tweets_diff[\"Content\"].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets_diff.info(show_counts=True))\n",
    "# print(shape(tweets_diff))\n",
    "\n",
    "# for i in tweets_diff[\"Content\"]:\n",
    "#     if \"It isnâ€™t the type of fear\" in i:\n",
    "#         print(i)\n",
    "\n",
    "tweets_diff[\"Content\"].tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example reg ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_list = [\"this has? a question.\", \"this has  two spaces.\", \"this one with an exclamation!\"]\n",
    "my_df = pd.DataFrame(my_list, columns=[\"Text\"])\n",
    "print(my_df)\n",
    "print()\n",
    "\n",
    "for i, text in enumerate(my_df[\"Text\"]):\n",
    "    my_df.loc[i, \"Text\"] = re.sub(r'\\?', r' ?', text)\n",
    "\n",
    "for i, text in enumerate(my_df[\"Text\"]):\n",
    "    my_df.loc[i, \"Text\"] = re.sub(r'!', r' !', text)\n",
    "\n",
    "for i, text in enumerate(my_df[\"Text\"]):\n",
    "    my_df.loc[i, \"Text\"] = re.sub(r'  ', r' ', text)\n",
    "\n",
    "print(my_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a quick checker:\n",
    "\n",
    "# NOTE: Some rows have been deleted and no longer exist.\n",
    "#tweets_data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(tweets_data.loc[260961])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning tweets: find rows with safewords, keep those in one dataframe, leave the rest in the original dataframe\n",
    "\n",
    "# Go through the dataframe looking at each cleaned tweet, \n",
    "# if one of the safewords is in the tweet, put that row in the index and increment the counter.\n",
    "# for i, tweet in enumerate(tweets_for_eval[\"ContentClean\"]):\n",
    "#     if \"kabul\" in tweet.lower():\n",
    "#         afg_index.append(i)\n",
    "#         word_finder_counter +=1\n",
    "#     elif \"airport\" in tweet.lower():\n",
    "#         afg_index.append(i)\n",
    "#         word_finder_counter +=1\n",
    "#     elif \"interpreter\" in tweet.lower():\n",
    "#         afg_index.append(i)\n",
    "#         word_finder_counter +=1\n",
    "#     elif \"interpreters\" in tweet.lower():\n",
    "#         afg_index.append(i)\n",
    "#         word_finder_counter +=1\n",
    "#     elif \"evacuation\" in tweet.lower():\n",
    "#         afg_index.append(i)\n",
    "#         word_finder_counter +=1\n",
    "#     elif \"allies\" in tweet.lower():\n",
    "#         afg_index.append(i)\n",
    "#         word_finder_counter +=1\n",
    "#     else:\n",
    "#         eval_index.append(i)\n",
    "#         eval_counter += 1\n",
    "\n",
    "# # remove duplicates from the indexes.\n",
    "# afg_index = list(dict.fromkeys(afg_index))\n",
    "# eval_index = list(dict.fromkeys(eval_index))\n",
    "\n",
    "# # Print some data checks\n",
    "# print(\"Number of values in Safeword_index:\", len(afg_index))\n",
    "# print(\"Number of rows with safewords:\", word_finder_counter)\n",
    "# print(\"Number of rows without safewords:\", eval_counter)\n",
    "# print(\"All rows accounted for:\", word_finder_counter + eval_counter == orig_rows)\n",
    "# print()\n",
    "\n",
    "# NOTE: BAD: this is really slow\n",
    "# Instead, create a column to flag the desired rows and copy from that\n",
    "# for i, tweet in enumerate(tweets_for_eval[\"ContentClean\"]):\n",
    "#     if i in afg_index:\n",
    "#         tweets_keep.append(tweets_for_eval[i])\n",
    "#         tweets_for_eval.drop(i, inplace=True)\n",
    "#     if i % 10000 == 0:\n",
    "#         print(\"Row:\", i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GOOD FUNCTION: breaks the text into tokens, replacing emoji with text.\n",
    "\n",
    "# emoji\n",
    "# TweetTokenizer\n",
    "# re\n",
    "\n",
    "print(tweets_clean.shape)\n",
    "\n",
    "def to_token (df, text_col=\"ContentClean\", token_col=\"contentToken\", indx_warning=True):\n",
    "    tknzr = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True, match_phone_numbers=True)\n",
    "    my_tokens = []\n",
    "\n",
    "    if indx_warning == True:\n",
    "        q = input(\"This function resets the index. To proceed, type: 'Y':\")\n",
    "        if q.lower() != \"y\":\n",
    "            return \"Error: Dataframe cannot be reindexed.\"\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    for i, text in enumerate(df[text_col]):\n",
    "        etext = emoji.demojize(text)\n",
    "        my_tokens.append([tknzr.tokenize(i) for i in sent_tokenize(etext)])\n",
    "\n",
    "    df.insert(loc=4, column=token_col, value=my_tokens)\n",
    "\n",
    "to_token (tweets_clean)\n",
    "\n",
    "print(len(my_tokens))\n",
    "print(tweets_clean.shape)\n",
    "tweets_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puts each sentence in a list, then tokenizes each list\n",
    "text = \"Since when does it take that much money to put someone into an apt? Biden is spending our tax dollars for everyone except Americans. Heâ€™s like a kid in an ice cream shop ~ ~ Biden Asks Congress For $6.4 Billion in Taxpayer Money to Resettle 95,000 Afghans https://t.co/wkkmuArH9w https://t.co/bJtRukWa6M\"\n",
    "parsed = sent_tokenize(text)\n",
    "token1 = [word_tokenize(i) for i in sent_tokenize(text)]\n",
    "token2 = word_tokenize(text)\n",
    "print(parsed)\n",
    "print(token1)\n",
    "print()\n",
    "print(token2)\n",
    "\n",
    "# To preserve :code: emoji translation format\n",
    "token1 = word_tokenize(text)\n",
    "token1 = [emoji.demojize(i) for i in token1]\n",
    "print(token1)\n",
    "\n",
    "# Shows that the average sentiment scores of two sentences independently \n",
    "# is NOT the same as the sentiment score of the two considered together.\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "txt = \"I strongly support your relocation to Afgh.\"\n",
    "print(sid.polarity_scores(txt))\n",
    "\n",
    "txt = \"I strongly support your relocation to Afgh. Love ya!\"\n",
    "print(sid.polarity_scores(txt))\n",
    "\n",
    "txt = [\"I strongly support your relocation to Afgh.\", \"Love ya!\"]\n",
    "print(sid.polarity_scores(txt))\n",
    "\n",
    "txt = [\"I strongly support your relocation to Afgh.\", \"Love ya!\"]\n",
    "scores = []\n",
    "for sent in txt:\n",
    "    a = sid.polarity_scores(sent)\n",
    "    scores.append(a['compound'])\n",
    "print(sum(scores)/len(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On puntuation\n",
    "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "\n",
    "I want to keep hastags. I want to remove @mentions and then lone @s, **in that order**.\n",
    "\n",
    "I want to keep ! and ?, but separated from the text.\n",
    "\n",
    "I may want to keep ' ' and \" \" as they could indicate sarcasm\n",
    "\n",
    "I may want to keep $, + and -, as they may actually have sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: KEEP for reference.\n",
    "\n",
    "# for i, text in enumerate(small_tweets[\"Content\"]):\n",
    "#     small_tweets.loc[i, \"Content\"] = re.sub(r'@\\w+', r' ', text)\n",
    "\n",
    "# a = re.sub(r'(\\s)#\\w+', r' ', 'Hello all please @at help #me   but#notme')\n",
    "# print(a)\n",
    "# what \"(\\s)#\\w+\" means:\n",
    "#   1st Capturing group (\\s) : match any white space character [\\r\\n\\t\\f ]\n",
    "#   # : matches the character # literally\n",
    "#   \\w+ : match any word character [a-zA-Z0-9_]\n",
    "#   + : Quantifier: Between one and unlimited times, as many times as possible, giving back as needed [greedy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigh, I think this whole thing can more or less be handled by the Tweet tokenizer.\n",
    "# It will separate everything appropriately (emoij-aware) and strip handels (@mentions).\n",
    "# After which I can remove the basic punctuation as stop words.\n",
    "\n",
    "#NOTE: This cell takes about 90min per expression on the ~250,000 dataset. Aaaaand I need to do it again, because I forgot to remove the  @.\n",
    "#NOTE: this type of regex replacements cannot be combined into the same enumerated \"for\" loop\n",
    "\n",
    "punct = '%&\\()*,./:;<=>[\\\\]^_`{|}~'\n",
    "\n",
    "for i, text in enumerate(tweets_data[\"Content\"]):\n",
    "    tweets_data.loc[i, \"Content\"] = re.sub(r'@\\w+', r' ', text)\n",
    "\n",
    "for i, text in enumerate(tweets_data[\"ContentClean\"]):\n",
    "    tweets_data.loc[i, \"ContentClean\"] = ' '.join(word.strip(punct) for word in text.split())\n",
    "\n",
    "for i, text in enumerate(tweets_data[\"ContentClean\"]):\n",
    "    tweets_data.loc[i, \"ContentClean\"] = re.sub(r'\\?', r' ?', text)\n",
    "\n",
    "for i, text in enumerate(tweets_data[\"ContentClean\"]):\n",
    "    tweets_data.loc[i, \"ContentClean\"] = re.sub(r'!', r' !', text)\n",
    "\n",
    "for i, text in enumerate(tweets_data[\"ContentClean\"]):\n",
    "    tweets_data.loc[i, \"ContentClean\"] = re.sub(r'  ', r' ', text)\n",
    "\n",
    "print(tweets_data[\"ContentClean\"].head())\n",
    "print(tweets_data[\"ContentClean\"].tail())\n",
    "\n",
    "tweets_data.to_csv(\"temp_clean.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2522cab69aef9135531abc74cfe3f2456cb406a72442e0865122b8d4f66eb9dc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
