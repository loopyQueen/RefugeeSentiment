{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGnlRWvkY-2c"
      },
      "source": [
        "code modified from: https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/#setup\n",
        "\n",
        "There are a lot of helpers that make using BERT easy with the Transformers library. I might want to also use [BertForSequenceClassification](https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification). \n",
        "\n",
        "I'll start with the basic [BertModel](https://huggingface.co/transformers/model_doc/bert.html#bertmodel) and build our sentiment classifier on top of it. \n",
        "\n",
        "# Sentiment Analysis with BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "w68CZpOwFoly",
        "outputId": "9c1a0321-1650-4224-cf9c-3c8dc8661ed3"
      },
      "outputs": [],
      "source": [
        "# import transformers\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "# from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "\n",
        "from torch import nn # optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "\n",
        "rcParams['figure.figsize'] = 12, 8\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the files that result from dataSplitBalance\n",
        "unbal_x_train = pd.read_csv(\"../dataBalSetsEcodes/unbal_x_train.csv\", header=0, index_col=0)\n",
        "unbal_x_val = pd.read_csv(\"../dataBalSetsEcodes/unbal_x_val.csv\", header=0, index_col=0)\n",
        "unbal_y_train = pd.read_csv(\"../dataBalancedSets/unbal_y_train.csv\", header=0, index_col=0)\n",
        "unbal_y_val = pd.read_csv(\"../dataBalancedSets/unbal_y_val.csv\", header=0, index_col=0)\n",
        "\n",
        "# And the test dataset\n",
        "x_test = pd.read_csv(\"../dataBalSetsEcodes/x_test.csv\", header=0, index_col=0)\n",
        "y_test = pd.read_csv(\"../dataBalancedSets/y_test_sent.csv\", header=0, index_col=0)\n",
        "\n",
        "print(\"Unbalanced:\")\n",
        "print(\"x-train:\", unbal_x_train.shape, \"x-val:\", unbal_x_val.shape, \"y-train:\", unbal_y_train.shape, \"y-val:\", unbal_y_val.shape)\n",
        "print(\"TEST DATA\")\n",
        "print(\"x-TEST:\", x_test.shape, \"y-TEST:\", y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "NJ6MhJYYBCwu",
        "outputId": "fc1bc767-990a-486d-bb10-2a80cea128b1"
      },
      "outputs": [],
      "source": [
        "\n",
        "!nvidia-smi --query-gpu=memory.free --format=csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How do we come up with all hyperparameters? The BERT authors have some recommendations for fine-tuning:\n",
        "\n",
        "- Batch size: 16, 32\n",
        "- Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
        "- Number of epochs: 2, 3, 4\n",
        "\n",
        "We're going to ignore the number of epochs recommendation but stick with the rest. Note that increasing the batch size reduces the training time significantly, but gives you lower accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DATASET = \"../data/tweets_clean.csv\"  # this is the full, labeled dataset for training.\n",
        "X_TRAIN = unbal_x_train\n",
        "Y_TRAIN = unbal_y_train\n",
        "X_VAL = unbal_x_val\n",
        "Y_VAL = unbal_y_val\n",
        "X_TEST = x_test\n",
        "Y_TEST = y_test\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "CONTENT_COL = \"ContentClean\"\n",
        "SCORE_COL = \"y_sent\"\n",
        "\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 8\n",
        "MAX_LEN = 150\n",
        "TRAIN_PERCENT = .80\n",
        "\n",
        "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
        "\n",
        "\n",
        "# original example: Initialcores are 1,2,3,4,5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(list(X_TRAIN.columns))\n",
        "print(list(Y_TRAIN.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Put the dfs together in the format needed for this notebook\n",
        "frames1 = [X_TRAIN, Y_TRAIN]\n",
        "df_train = pd.concat(frames1, axis=1, ignore_index=True) #comb horiz\n",
        "df_train.columns = ['id_stable', 'Date', 'ContentClean','trash', 'label_sent', 'y_sent', 'label_stance', 'y_stance']\n",
        "df_train.drop([\"trash\", 'label_sent', 'label_stance', 'y_stance'], axis = 1, inplace=True)\n",
        "\n",
        "frames2 = [X_VAL, Y_VAL]\n",
        "df_val = pd.concat(frames2, axis=1, ignore_index=True)\n",
        "df_val.columns = ['id_stable', 'Date', 'ContentClean','trash', 'label_sent', 'y_sent', 'label_stance', 'y_stance']\n",
        "df_val.drop([\"trash\", 'label_sent', 'label_stance', 'y_stance'], axis = 1, inplace=True)\n",
        "\n",
        "\n",
        "frames3 = [X_TEST, Y_TEST]\n",
        "df_test = pd.concat(frames3, axis=1, ignore_index=True)\n",
        "df_test.columns = ['id_stable', 'Date', 'ContentClean','trash', 'label_sent', 'y_sent', 'label_stance', 'y_stance']\n",
        "df_test.drop([\"trash\", 'label_sent', 'label_stance', 'y_stance'], axis = 1, inplace=True)\n",
        "\n",
        "print(df_train.shape)\n",
        "print(df_val.shape)\n",
        "print(df_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufzPdoTtNikq"
      },
      "source": [
        "## Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "mUKLyKc7I6Qp",
        "outputId": "81441fd4-74b2-4b5b-b4f1-76c41cb6ce6a"
      },
      "outputs": [],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_val.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "VA_wGSLQLKCh",
        "outputId": "9468ddba-47d0-46a8-d7bb-6ff51c4bdb8c"
      },
      "outputs": [],
      "source": [
        "df_train.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3cL_1qVn_6h"
      },
      "source": [
        "No missing values in the score and review texts. \n",
        "\n",
        "Do we have class imbalance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "Wwh_rW4Efhs3",
        "outputId": "e39b9955-3c5e-45f3-f960-38bfa03447c4"
      },
      "outputs": [],
      "source": [
        "sns.countplot(df_train[SCORE_COL])\n",
        "plt.xlabel('review score');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aHyGuTFgyPO"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Machine Learning models don't work with raw text. You need to convert text to numbers (of some sort). BERT requires: \n",
        "\n",
        "- Add special tokens to separate sentences and do classification\n",
        "- Pass sequences of constant length (introduce padding)\n",
        "- Create array of 0s (pad token) and 1s (real token) called *attention mask*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiLb-ltM-ZRz"
      },
      "source": [
        "Let's load a pre-trained [BertTokenizer](https://huggingface.co/transformers/model_doc/bert.html#berttokenizer):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3AfJSZ8NNLF"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME) # BertForSequenceClassification changed in two places below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waKjYxTDuaWt"
      },
      "source": [
        "### Choosing Sequence Length\n",
        "\n",
        "BERT works with fixed-length sequences. We'll use a simple strategy to choose the max length. Let's store the token length of each review:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUnE5CT9hbeZ"
      },
      "outputs": [],
      "source": [
        "token_lens = []\n",
        "\n",
        "for txt in df_train[CONTENT_COL]:\n",
        "  tokens = tokenizer.encode(txt, max_length=512)\n",
        "  token_lens.append(len(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI4goUrHf6da"
      },
      "source": [
        "and plot the distribution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "SzE1j4jxmUtd",
        "outputId": "cf03f40b-88a7-43b0-bc2c-32eb6e16c935"
      },
      "outputs": [],
      "source": [
        "sns.distplot(token_lens)\n",
        "plt.xlim([0, 256]);\n",
        "plt.xlabel('Token count');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW6ajl30t6du"
      },
      "source": [
        "Most of the reviews seem to contain fewer than 100 tokens, but we'll be on the safe side and choose a maximum length of 150 (set above, as hyperparameter)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvvcoU6nurHy"
      },
      "source": [
        "We have all building blocks required to create a PyTorch dataset. Let's do it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2BPgRJ7YBK0"
      },
      "outputs": [],
      "source": [
        "class GPReviewDataset(Dataset):\n",
        "\n",
        "  def __init__(self, reviews, targets, tokenizer, max_len):\n",
        "    self.reviews = reviews\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    review = str(self.reviews[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      review,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'review_text': review,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2uwsvCYqDJK"
      },
      "source": [
        "The tokenizer is doing most of the heavy lifting for us. We also return the review texts, so it'll be easier to evaluate the predictions from our model. Let's split the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "xz3ZOQXVPCwh",
        "outputId": "dd8d2844-3b22-425d-dc40-725f7f46e52a"
      },
      "outputs": [],
      "source": [
        "df_train.shape, df_val.shape, df_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4tQ1x-vqNab"
      },
      "source": [
        "We also need to create a couple of data loaders. Here's a helper function to do it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEGqcvkuOuTX"
      },
      "outputs": [],
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = GPReviewDataset(\n",
        "    reviews=df[CONTENT_COL].to_numpy(),\n",
        "    targets=df[SCORE_COL].to_numpy(),  \n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=0\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vODDxMKsPHqI"
      },
      "outputs": [],
      "source": [
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "# df_full = []\n",
        "# full_data_loader = create_data_loader(df_full, tokenizer, MAX_LEN, BATCH_SIZE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H63Y-TjyRC7S"
      },
      "source": [
        "## Sentiment Classification with BERT and Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "440Nd31VTHER"
      },
      "source": [
        "Let's load the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0P41FayISNRI"
      },
      "outputs": [],
      "source": [
        "bert_model = BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o_NiS3WgOFf"
      },
      "source": [
        "We can use all of this knowledge to create a classifier that uses the BERT model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_mRflxPl32F"
      },
      "outputs": [],
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(SentimentClassifier, self).__init__()\n",
        "    self.bert = BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict = False)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    _, pooled_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJg8m3NQJahc"
      },
      "source": [
        "Our classifier delegates most of the heavy lifting to the BertModel. We use a dropout layer for some regularization and a fully-connected layer for our output. Note that we're returning the raw output of the last layer since that is required for the cross-entropy loss function in PyTorch to work.\n",
        "\n",
        "This should work like any other PyTorch model. Let's create an instance and move it to the GPU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0yQnuSFsjDp"
      },
      "outputs": [],
      "source": [
        "model = SentimentClassifier(len(class_names))\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9xikRdtRN1N"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76g7FV85H-T8"
      },
      "source": [
        "To reproduce the training procedure from the BERT paper, we'll use the [AdamW](https://huggingface.co/transformers/main_classes/optimizer_schedules.html#adamw) optimizer provided by Hugging Face. It corrects weight decay, so it's similar to the original paper. We'll also use a linear scheduler with no warmup steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5v-ArJ2fCCcU"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8522g7JIu5J"
      },
      "source": [
        "Let's continue with writing a helper function for training our model for one epoch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzl9UhuNx1_Q"
      },
      "outputs": [],
      "source": [
        "def train_epoch(\n",
        "  model, \n",
        "  data_loader, \n",
        "  loss_fn, \n",
        "  optimizer, \n",
        "  device, \n",
        "  scheduler, \n",
        "  n_examples\n",
        "):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  for d in data_loader:\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4PniYIte0fr"
      },
      "source": [
        "Training the model should look familiar, except for two things. The scheduler gets called every time a batch is fed to the model. We're avoiding exploding gradients by clipping the gradients of the model using [clip_grad_norm_](https://pytorch.org/docs/stable/nn.html#clip-grad-norm).\n",
        "\n",
        "Let's write another one that helps us evaluate the model on a given data loader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXeRorVGIKre"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_rdSDBHhhCh"
      },
      "source": [
        "Using those two, we can write our training loop. We'll also store the training history:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "id": "1zhHoFNsxufs",
        "outputId": "2f11710a-700e-4933-b57e-5d50e5ed1f78"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# around 11 min per run\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,    \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    device, \n",
        "    scheduler, \n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn, \n",
        "    device, \n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "\n",
        "if val_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "    best_accuracy = val_acc\n",
        "\n",
        "# if val_acc > best_accuracy:\n",
        "    # torch.save(model.bert.state_dict(), 'best_model_state.bin')\n",
        "    # model.bert.config.to_json_file('best_model_config.json')  #GINA2\n",
        "    # tokenizer.save_vocabulary('./') #GINA2\n",
        "    # best_accuracy = val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r8-5zWsiVur"
      },
      "source": [
        "Note that we're storing the state of the best model, indicated by the highest validation accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLQf52c7fbzr"
      },
      "source": [
        "Whoo, this took some time! We can look at the training vs validation accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "-FWG7kBm372V",
        "outputId": "9dd7f8cf-8f36-4280-dfff-bdaa8b9b89f2"
      },
      "outputs": [],
      "source": [
        "plt.plot(list(map(lambda x:x.item(), history['train_acc'])), label='train accuracy')\n",
        "plt.plot(list(map(lambda x: x.item(), history['val_acc'])), label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsHqkLAuf8pv"
      },
      "source": [
        "The training accuracy starts to approach 100% after 10 epochs or so. You might try to fine-tune the parameters a bit more, but this will be good enough for us.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoGUH8VZ-pPQ"
      },
      "outputs": [],
      "source": [
        "# # class_names = ['negative', 'neutral', 'positive']\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# model = SentimentClassifier(len(class_names))\n",
        "# model.load_state_dict(torch.load('best_model_state.bin'))\n",
        "# model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3HZb3NWFtFf"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "So how good is our model on predicting sentiment? Let's start by calculating the accuracy on the test data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "jS3gJ_qBEljD",
        "outputId": "21f968b6-fd29-4e74-dee0-8dc9eacd301e"
      },
      "outputs": [],
      "source": [
        "test_acc, _ = eval_model(\n",
        "  model,\n",
        "  test_data_loader,\n",
        "  loss_fn,\n",
        "  device,\n",
        "  len(df_test)\n",
        ")\n",
        "\n",
        "test_acc.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdQ7-ylCj8Gd"
      },
      "source": [
        "We'll define a helper function to get the predictions from our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgR6MuNS8jr_"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  \n",
        "  review_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      texts = d[\"review_text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "      review_texts.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(probs)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return review_texts, predictions, prediction_probs, real_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkbnBTI7kd_y"
      },
      "source": [
        "This is similar to the evaluation function, except that we're storing the text of the reviews and the predicted probabilities (by applying the softmax on the model outputs):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHdPZr60-0c_"
      },
      "outputs": [],
      "source": [
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVwoVij2lC7F"
      },
      "source": [
        "Let's have a look at the classification report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "L8a9_8-ND3Is",
        "outputId": "9b2c48cc-b62e-41f3-dba5-af90457a37de"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, y_pred, target_names=class_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFAekw3mmWUi"
      },
      "source": [
        "Looks like it is really hard to classify neutral tweets. \n",
        "\n",
        "We'll continue with the confusion matrix of the test data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "6d1qxsc__DTh",
        "outputId": "14b8839c-4e14-430c-b185-46b09bd4231e"
      },
      "outputs": [],
      "source": [
        "def show_confusion_matrix(confusion_matrix):\n",
        "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=18)\n",
        "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right', fontsize=18)\n",
        "  plt.title(\"BERT-base,\\nPre-Tuned Model, Unbalanced Data\\n\", fontsize=20)\n",
        "  plt.ylabel('Expected', fontsize=20)\n",
        "  plt.xlabel('Predicted', fontsize=20);\n",
        "  \n",
        "fig, ax = plt.subplots(figsize=(5, 4))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "show_confusion_matrix(df_cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_l = y_pred.tolist()\n",
        "y_test_l = y_test.tolist()\n",
        "\n",
        "print(len(y_pred_l))\n",
        "print(y_pred_l.count(0), y_pred_l.count(1), y_pred_l.count(2))\n",
        "print()\n",
        "print(len(y_test_l))\n",
        "print(y_test_l.count(0), y_test_l.count(1), y_test_l.count(2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx0U7oNsnZ3A"
      },
      "source": [
        "This confirms that our model is having difficulty classifying neutral and positive reviews. It mistakes those for the other classes at a roughly equal frequency.\n",
        "\n",
        "That's a good overview of the performance of our model. But let's have a look at an example from our test data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iANBiY3sLo-K"
      },
      "outputs": [],
      "source": [
        "idx = 2\n",
        "\n",
        "review_text = y_review_texts[idx]\n",
        "true_sentiment = y_test[idx]\n",
        "pred_df = pd.DataFrame({\n",
        "  'class_names': class_names,\n",
        "  'values': y_pred_probs[idx]\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "-8D0rb1yfnv4",
        "outputId": "3c2aa437-9c0d-4421-adf6-9d12e87f4a83"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\".join(wrap(review_text)))\n",
        "print()\n",
        "print(f'True sentiment: {class_names[true_sentiment]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7hj_IZFnn2X"
      },
      "source": [
        "Now we can look at the confidence of each sentiment of our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "qj4d8lZyMkhf",
        "outputId": "3e1e4f5d-3ae0-41bd-8ddc-348e85c13e98"
      },
      "outputs": [],
      "source": [
        "sns.barplot(x='values', y='class_names', data=pred_df, orient='h')\n",
        "plt.ylabel('sentiment')\n",
        "plt.xlabel('probability')\n",
        "plt.xlim([0, 1]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WL5pDmvFyaU"
      },
      "source": [
        "### Predicting on Raw Text\n",
        "\n",
        "Let's use our model to predict the sentiment of some raw text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEPi7zQRsDhH"
      },
      "outputs": [],
      "source": [
        "review_text = \"I hate completing my todos! Worst app ever!!!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaN4RnqMnxYw"
      },
      "source": [
        "We have to use the tokenizer to encode the text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zA5Or4D2sLc9"
      },
      "outputs": [],
      "source": [
        "encoded_review = tokenizer.encode_plus(\n",
        "  review_text,\n",
        "  max_length=MAX_LEN,\n",
        "  add_special_tokens=True,\n",
        "  return_token_type_ids=False,\n",
        "  pad_to_max_length=True,\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "et8xlDrKpH60"
      },
      "source": [
        "Let's get the predictions from our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "Qr_t3rUksumr",
        "outputId": "4a69d750-c56a-40c1-822a-0b3e7df16b3e"
      },
      "outputs": [],
      "source": [
        "input_ids = encoded_review['input_ids'].to(device)\n",
        "attention_mask = encoded_review['attention_mask'].to(device)\n",
        "\n",
        "output = model(input_ids, attention_mask)\n",
        "_, prediction = torch.max(output, dim=1)\n",
        "\n",
        "print(f'Review text: {review_text}')\n",
        "print(f'Sentiment  : {class_names[prediction]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf39tauBa2V2"
      },
      "source": [
        "## References\n",
        "\n",
        "- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\n",
        "- [L11 Language Models - Alec Radford (OpenAI)](https://www.youtube.com/watch?v=BnpB3GrpsfM)\n",
        "- [The Illustrated BERT, ELMo, and co.](https://jalammar.github.io/illustrated-bert/)\n",
        "- [BERT Fine-Tuning Tutorial with PyTorch](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)\n",
        "- [How to Fine-Tune BERT for Text Classification?](https://arxiv.org/pdf/1905.05583.pdf)\n",
        "- [Huggingface Transformers](https://huggingface.co/transformers/)\n",
        "- [BERT Explained: State of the art language model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "08.sentiment-analysis-with-bert.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "2522cab69aef9135531abc74cfe3f2456cb406a72442e0865122b8d4f66eb9dc"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
