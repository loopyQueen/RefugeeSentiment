{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import transformers\n",
    "#from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should match the model used for training:\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'  # Should match the model used for training\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "MAX_LEN = 160\n",
    "class_names = ['negative', 'neutral', 'positive']\n",
    "# class_names = ['neutral', 'negative', 'positive']\n",
    "\n",
    "\n",
    "# Should be the path to the full dataset you want to apply the tuned model to:\n",
    "data = pd.read_csv(\"../data/all_unlabeled_tweets.csv\", index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "\n",
    "  def __init__(self, n_classes):\n",
    "    super(SentimentClassifier, self).__init__()\n",
    "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict = False)\n",
    "    self.drop = nn.Dropout(p=0.3)\n",
    "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "  \n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    _, pooled_output = self.bert(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "    output = self.drop(pooled_output)\n",
    "    return self.out(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SentimentClassifier(len(class_names))\n",
    "model.load_state_dict(torch.load('best_model_state.bin'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\rnocker\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2285: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral : Refugee is another name for terrorist\n",
      "positive : we have an obligation to them; I'm glad we are doing something for these people.\n",
      "negative : I don't care what happens to the translators.\n",
      "neutral : 1000 refugees entered Iowa.\n"
     ]
    }
   ],
   "source": [
    "def infer_model(review_texts):\n",
    "    res = []\n",
    "    for review_text in review_texts:\n",
    "        encoded_review = tokenizer.encode_plus(\n",
    "        review_text,\n",
    "        max_length=MAX_LEN,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "        )\n",
    "        input_ids = encoded_review['input_ids'].to(device)\n",
    "        attention_mask = encoded_review['attention_mask'].to(device)\n",
    "\n",
    "        output = model(input_ids, attention_mask)\n",
    "        _, prediction = torch.max(output, dim=1)\n",
    "\n",
    "        res.append(class_names[prediction])\n",
    "    return res\n",
    "\n",
    "\n",
    "def infer_model2(review_text):\n",
    "    encoded_review = tokenizer.encode_plus(\n",
    "    review_text,\n",
    "    max_length=MAX_LEN,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=False,\n",
    "    pad_to_max_length=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    "    )\n",
    "    input_ids = encoded_review['input_ids'].to(device)\n",
    "    attention_mask = encoded_review['attention_mask'].to(device)\n",
    "\n",
    "    output = model(input_ids, attention_mask)\n",
    "    _, prediction = torch.max(output, dim=1)\n",
    "    # print(output[0][0].item(), output[0][1].item(), output[0][2].item(), _, prediction)\n",
    "    return class_names[prediction], output[0][0].item(), output[0][1].item(), output[0][2].item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_texts = [\"Refugee is another name for terrorist\",\"we have an obligation to them; I'm glad we are doing something for these people.\", \"I don't care what happens to the translators.\", \"1000 refugees entered Iowa.\"]\n",
    "predictions = infer_model(review_texts)\n",
    "for i, review_text in enumerate(review_texts):\n",
    "    print(f'{predictions[i]} : {review_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run it on the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rnocker\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2285: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_stable</th>\n",
       "      <th>Date</th>\n",
       "      <th>Content</th>\n",
       "      <th>ContentClean</th>\n",
       "      <th>Flag</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>neutral</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19167</td>\n",
       "      <td>2022-02-11 00:54:35+00:00</td>\n",
       "      <td>What are we doing to secure our #Afghan SIVs, ...</td>\n",
       "      <td>What are we doing to secure our #Afghan SIVs, ...</td>\n",
       "      <td>no</td>\n",
       "      <td>negative</td>\n",
       "      <td>-2.569971</td>\n",
       "      <td>6.064625</td>\n",
       "      <td>-3.534876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>174397</td>\n",
       "      <td>2021-08-14 00:51:43+00:00</td>\n",
       "      <td>Please help get this guidance out there: \\nhel...</td>\n",
       "      <td>Please help get this guidance out there: help ...</td>\n",
       "      <td>no</td>\n",
       "      <td>neutral</td>\n",
       "      <td>6.125570</td>\n",
       "      <td>-2.939024</td>\n",
       "      <td>-2.730552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>166369</td>\n",
       "      <td>2021-08-16 10:34:37+00:00</td>\n",
       "      <td>A decade ago, We saw biggest humanitarian &amp;amp...</td>\n",
       "      <td>A decade ago, We saw biggest humanitarian &amp; re...</td>\n",
       "      <td>no</td>\n",
       "      <td>negative</td>\n",
       "      <td>-2.724625</td>\n",
       "      <td>5.823037</td>\n",
       "      <td>-3.144567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>133951</td>\n",
       "      <td>2021-08-20 20:42:10+00:00</td>\n",
       "      <td>Chaired #UnitingChurch leaders meeting Friday ...</td>\n",
       "      <td>Chaired #UnitingChurch leaders meeting Friday ...</td>\n",
       "      <td>no</td>\n",
       "      <td>negative</td>\n",
       "      <td>-2.031280</td>\n",
       "      <td>6.152158</td>\n",
       "      <td>-3.699427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>171885</td>\n",
       "      <td>2021-08-15 11:16:10+00:00</td>\n",
       "      <td>The tragedy unfolding in Afghanistan is terrif...</td>\n",
       "      <td>The tragedy unfolding in Afghanistan is terrif...</td>\n",
       "      <td>no</td>\n",
       "      <td>negative</td>\n",
       "      <td>-3.322519</td>\n",
       "      <td>6.888148</td>\n",
       "      <td>-3.385778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>98984</td>\n",
       "      <td>2021-08-31 16:01:44+00:00</td>\n",
       "      <td>@votesamuelwill1 All Americans living in Afgha...</td>\n",
       "      <td>All Americans living in Afghanistan were told ...</td>\n",
       "      <td>no</td>\n",
       "      <td>negative</td>\n",
       "      <td>-3.039819</td>\n",
       "      <td>6.480907</td>\n",
       "      <td>-3.313448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>189052</td>\n",
       "      <td>2021-06-30 19:03:47+00:00</td>\n",
       "      <td>@SenatorWicker Thank you so much Mr.wicker.\\nP...</td>\n",
       "      <td>Thank you so much Mr.wicker. Please heard thos...</td>\n",
       "      <td>no</td>\n",
       "      <td>negative</td>\n",
       "      <td>-2.769158</td>\n",
       "      <td>6.282814</td>\n",
       "      <td>-2.657940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>28950</td>\n",
       "      <td>2022-01-06 14:30:28+00:00</td>\n",
       "      <td>Afghan refugee who 'raped and murdered' 13-yea...</td>\n",
       "      <td>Afghan refugee who 'raped and murdered' 13-yea...</td>\n",
       "      <td>no</td>\n",
       "      <td>negative</td>\n",
       "      <td>-2.422650</td>\n",
       "      <td>6.342859</td>\n",
       "      <td>-3.491925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>109872</td>\n",
       "      <td>2021-08-27 11:11:31+00:00</td>\n",
       "      <td>@JBowers56 @GamblerJam @iowahawkblog That's a ...</td>\n",
       "      <td>That's a lot of supposition without any eviden...</td>\n",
       "      <td>no</td>\n",
       "      <td>negative</td>\n",
       "      <td>-3.183186</td>\n",
       "      <td>6.004797</td>\n",
       "      <td>-3.425128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>71603</td>\n",
       "      <td>2021-09-17 19:16:02+00:00</td>\n",
       "      <td>@RepRosendale MT is welcoming to others, and w...</td>\n",
       "      <td>MT is welcoming to others, and willing to help...</td>\n",
       "      <td>no</td>\n",
       "      <td>positive</td>\n",
       "      <td>-3.390904</td>\n",
       "      <td>0.869125</td>\n",
       "      <td>2.092019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_stable                       Date  \\\n",
       "0      19167  2022-02-11 00:54:35+00:00   \n",
       "1     174397  2021-08-14 00:51:43+00:00   \n",
       "2     166369  2021-08-16 10:34:37+00:00   \n",
       "3     133951  2021-08-20 20:42:10+00:00   \n",
       "4     171885  2021-08-15 11:16:10+00:00   \n",
       "5      98984  2021-08-31 16:01:44+00:00   \n",
       "6     189052  2021-06-30 19:03:47+00:00   \n",
       "7      28950  2022-01-06 14:30:28+00:00   \n",
       "8     109872  2021-08-27 11:11:31+00:00   \n",
       "9      71603  2021-09-17 19:16:02+00:00   \n",
       "\n",
       "                                             Content  \\\n",
       "0  What are we doing to secure our #Afghan SIVs, ...   \n",
       "1  Please help get this guidance out there: \\nhel...   \n",
       "2  A decade ago, We saw biggest humanitarian &amp...   \n",
       "3  Chaired #UnitingChurch leaders meeting Friday ...   \n",
       "4  The tragedy unfolding in Afghanistan is terrif...   \n",
       "5  @votesamuelwill1 All Americans living in Afgha...   \n",
       "6  @SenatorWicker Thank you so much Mr.wicker.\\nP...   \n",
       "7  Afghan refugee who 'raped and murdered' 13-yea...   \n",
       "8  @JBowers56 @GamblerJam @iowahawkblog That's a ...   \n",
       "9  @RepRosendale MT is welcoming to others, and w...   \n",
       "\n",
       "                                        ContentClean Flag sentiment   neutral  \\\n",
       "0  What are we doing to secure our #Afghan SIVs, ...   no  negative -2.569971   \n",
       "1  Please help get this guidance out there: help ...   no   neutral  6.125570   \n",
       "2  A decade ago, We saw biggest humanitarian & re...   no  negative -2.724625   \n",
       "3  Chaired #UnitingChurch leaders meeting Friday ...   no  negative -2.031280   \n",
       "4  The tragedy unfolding in Afghanistan is terrif...   no  negative -3.322519   \n",
       "5  All Americans living in Afghanistan were told ...   no  negative -3.039819   \n",
       "6  Thank you so much Mr.wicker. Please heard thos...   no  negative -2.769158   \n",
       "7  Afghan refugee who 'raped and murdered' 13-yea...   no  negative -2.422650   \n",
       "8  That's a lot of supposition without any eviden...   no  negative -3.183186   \n",
       "9  MT is welcoming to others, and willing to help...   no  positive -3.390904   \n",
       "\n",
       "   negative  positive  \n",
       "0  6.064625 -3.534876  \n",
       "1 -2.939024 -2.730552  \n",
       "2  5.823037 -3.144567  \n",
       "3  6.152158 -3.699427  \n",
       "4  6.888148 -3.385778  \n",
       "5  6.480907 -3.313448  \n",
       "6  6.282814 -2.657940  \n",
       "7  6.342859 -3.491925  \n",
       "8  6.004797 -3.425128  \n",
       "9  0.869125  2.092019  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'], data['negative'], data['neutral'], data['positive'] = zip(*data.ContentClean.apply(infer_model2))\n",
    "\n",
    "data.to_csv(\"../dataBert/temp_full_pred.csv\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_stable</th>\n",
       "      <th>Date</th>\n",
       "      <th>Content</th>\n",
       "      <th>ContentClean</th>\n",
       "      <th>Flag</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>neutral</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200079</th>\n",
       "      <td>106134</td>\n",
       "      <td>2021-08-28 15:34:42+00:00</td>\n",
       "      <td>Did you miss our Operation Welcome Afghan Alli...</td>\n",
       "      <td>Did you miss our Operation Welcome Afghan Alli...</td>\n",
       "      <td>no</td>\n",
       "      <td>neutral</td>\n",
       "      <td>4.793899</td>\n",
       "      <td>-3.113419</td>\n",
       "      <td>-1.934890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200080</th>\n",
       "      <td>94022</td>\n",
       "      <td>2021-09-02 11:59:32+00:00</td>\n",
       "      <td>£250,000 will be available through the Scottis...</td>\n",
       "      <td>£250,000 will be available through the Scottis...</td>\n",
       "      <td>no</td>\n",
       "      <td>positive</td>\n",
       "      <td>-2.725789</td>\n",
       "      <td>-3.391924</td>\n",
       "      <td>5.098685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200081</th>\n",
       "      <td>182463</td>\n",
       "      <td>2021-07-26 12:30:15+00:00</td>\n",
       "      <td>@Ho34980636 @AmrullahSaleh2 Pakistan is dying ...</td>\n",
       "      <td>Pakistan is dying state and Afghanistan is ris...</td>\n",
       "      <td>no</td>\n",
       "      <td>neutral</td>\n",
       "      <td>6.577663</td>\n",
       "      <td>-3.048520</td>\n",
       "      <td>-3.468530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200082</th>\n",
       "      <td>95209</td>\n",
       "      <td>2021-09-01 21:40:38+00:00</td>\n",
       "      <td>Pentagon chief says SIV program was not design...</td>\n",
       "      <td>Pentagon chief says SIV program was not design...</td>\n",
       "      <td>no</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.948322</td>\n",
       "      <td>3.920422</td>\n",
       "      <td>-4.929651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200083</th>\n",
       "      <td>83631</td>\n",
       "      <td>2021-09-08 05:30:05+00:00</td>\n",
       "      <td>[ Stigmabase IE ] Northern Ireland Executive a...</td>\n",
       "      <td>[ Stigmabase IE ] Northern Ireland Executive a...</td>\n",
       "      <td>no</td>\n",
       "      <td>neutral</td>\n",
       "      <td>4.083866</td>\n",
       "      <td>-2.601464</td>\n",
       "      <td>-1.374184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_stable                       Date  \\\n",
       "200079     106134  2021-08-28 15:34:42+00:00   \n",
       "200080      94022  2021-09-02 11:59:32+00:00   \n",
       "200081     182463  2021-07-26 12:30:15+00:00   \n",
       "200082      95209  2021-09-01 21:40:38+00:00   \n",
       "200083      83631  2021-09-08 05:30:05+00:00   \n",
       "\n",
       "                                                  Content  \\\n",
       "200079  Did you miss our Operation Welcome Afghan Alli...   \n",
       "200080  £250,000 will be available through the Scottis...   \n",
       "200081  @Ho34980636 @AmrullahSaleh2 Pakistan is dying ...   \n",
       "200082  Pentagon chief says SIV program was not design...   \n",
       "200083  [ Stigmabase IE ] Northern Ireland Executive a...   \n",
       "\n",
       "                                             ContentClean Flag sentiment  \\\n",
       "200079  Did you miss our Operation Welcome Afghan Alli...   no   neutral   \n",
       "200080  £250,000 will be available through the Scottis...   no  positive   \n",
       "200081  Pakistan is dying state and Afghanistan is ris...   no   neutral   \n",
       "200082  Pentagon chief says SIV program was not design...   no  negative   \n",
       "200083  [ Stigmabase IE ] Northern Ireland Executive a...   no   neutral   \n",
       "\n",
       "         neutral  negative  positive  \n",
       "200079  4.793899 -3.113419 -1.934890  \n",
       "200080 -2.725789 -3.391924  5.098685  \n",
       "200081  6.577663 -3.048520 -3.468530  \n",
       "200082  0.948322  3.920422 -4.929651  \n",
       "200083  4.083866 -2.601464 -1.374184  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2522cab69aef9135531abc74cfe3f2456cb406a72442e0865122b8d4f66eb9dc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
